
<!DOCTYPE HTML "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
    width: 980px;
}
h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1, h2, h3 {
    text-align: center;
}
h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
.paper-title {
    padding: 16px 0px 16px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-6 {
     width: 16.6%;
     float: left;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.row, .author-row, .affil-row {
     overflow: auto;
}
.author-row, .affil-row {
    font-size: 20px;
}
.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 18px;
}
.affil-row {
    margin-top: 16px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.text-justify {
    text-align: justify;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    /*font-style: italic;*/
    color: rgb(73, 73, 73);
    text-align: center;
    margin-top: 15px;
    margin-bottom: 12px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    justify-content: space-around;
    padding: 0;
    margin: 0;
    list-style: none;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}

.insight {
  background-color: #EEEEEE;
  padding-right: 100px;
  padding-left: 100px;
  padding-top: 20px;
  padding-bottom: 20px;
}

.supp-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 18px;
  width: 90px;
  font-weight: 400;
}

.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}
.paper-btn:hover {
    opacity: 0.85;
}
.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}
.venue {
    color: #1367a7;
}



.topnav {
  overflow: hidden;
  background-color: #EEEEEE;
}

.topnav a {
  float: left;
  color: black;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 16px;
}
.controls {
  margin-bottom: 10px;
  margin-top: 20px;
}
.left-controls {
  display: inline-block;
  vertical-align: top;
  width: 80%;
}
.right-controls {
  display: inline-block;
  vertical-align: top;
  width: 19%;
  text-align: right;
}

.render_window {
    display: inline-block;
    vertical-align: middle;
    box-shadow: 0px 0px 0px black;
    margin-right: 20px;
    margin-bottom: 20px;
    width: calc(33% - 20px);
}

</style>


<div class="topnav" id="myTopnav">
  <a href="https://www.nvidia.com/"><img width="100%" src="assets/nvidia.svg"></a>
  <a href="https://www.nvidia.com/en-us/research/"><strong>NVIDIA Research</strong></a>
  <a href="https://nv-tlabs.github.io/" ><strong>Toronto AI Lab</strong></a>
</div>

<!-- End : Google Analytics Code -->
<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
    <title>How Much More Data Do I Need? Estimating Requirements For Downstream Tasks</title>
    <meta property="og:description" content="How Much More Data Do I Need? Estimating Requirements For Downstream Tasks"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<!--
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6HHDEXF452"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-6HHDEXF452');
</script>
-->

</head>


 <body>
<div class="container">
    <div class="paper-title">
      <h1>How Much More Data Do I Need? Estimating Requirements For Downstream Tasks</h1>
    </div>

    
    <div id="authors">
        <div class="author-row">
            <div class="col-5 text-center"><a href="https://rafidrm.github.io/">Rafid Mahmood</a><sup>1*</sup></div>
            <div class="col-5 text-center"><a href="https://cs.toronto.edu/~jlucas/">James Lucas</a><sup>1,2,3</sup></div>
            <div class="col-5 text-center"><a href="https://cs.toronto.edu/~davidj/">David Acuna</a><sup>1,2,3</sup></div>
            <div class="col-5 text-center"><a href="https://scholar.google.ca/citations?user=8q2ISMIAAAAJ&hl=en/">Daiqing Li</a><sup>1</sup></div>
            <div class="col-5 text-center"><a href="https://cs.toronto.edu/~jphilion/">Jonah Philion</a><sup>1,2,3</sup></div>
            <div class="col-4 text-center"><a href="https://alvarezlopezjosem.github.io/">Jose M. Alvarez</a><sup>1</sup></div>
            <div class="col-4 text-center"><a href="https://chrisding.github.io/">Zhiding Yu</a><sup>1</sup></div>
            <div class="col-4 text-center"><a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a><sup>1,2,3</sup></div>
            <div class="col-4 text-center"><a href="https://www.cs.utoronto.edu/~law/">Marc T. Law</a><sup>1</sup></div>
        </div>

        <div class="affil-row">
            <div class="col-3 text-center"><sup>1</sup>NVIDIA</a></div>
            <div class="col-3 text-center"><sup>2</sup>University of Toronto</div>
            <div class="col-3 text-center"><sup>3</sup>Vector Institute</div>
        </div>
        <div class="affil-row">
            <div class="venue text-center"><b>CVPR 2022</b></div>
        </div>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="supp-btn" href="assets/main.pdf">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <a class="supp-btn" href="assets/CVPR_2022_poster.pdf">
                <span class="material-icons"> description </span> 
                  Poster
            </a>
            <a class="supp-btn" href="https://openaccess.thecvf.com/content/CVPR2022/html/Mahmood_How_Much_More_Data_Do_I_Need_Estimating_Requirements_for_CVPR_2022_paper.html">
                <span class="material-icons">description </span>
                  DOI
            </a>
            <a class="supp-btn" href="assets/bib.txt">
                <span class="material-icons"> description </span> 
                  BibTeX
            </a>
        </div></div>
    </div>

    <section id="teaser">
            <figure style="width: 90%;">
                <a href="assets/imagenet_regression_summary_v2.png">
                    <img width="55%" src="assets/imagenet_regression_summary_v2.png">
                </a>
                <a href="assets/Regression_Functions.PNG">
                    <img width="44%" src="assets/Regression_Functions.PNG" style="padding-bottom: 110px">
                </a>
                <p class="caption">
                    Extrapolating the learning curve <b>(Left Figure)</b> on ImageNet as a function
                    of data set size when given 10% of the data set (125, 000 images; dotted curves)
                    and 50% (600, 000 images; dashed curves) using four regression functions <b>(Right Table)</b>. 
                    The vertical lines show how much data is needed to
                    meet a desired 67% validation accuracy according to each dashed
                    curve. We observe:
                    <b>(1)</b> with a small initial set (i.e., 10%), all of the regression functions diverge
                    from the ground truth learning curve; 
                    <b>(2)</b> with enough data (i.e., 50%), the functions accurate extrapolate performance; but
                    <b>(3)</b> even a small extrapolation error can yield poor data estimates.
                    Although the functions have an error of 1-6% from the ground truth (67%
                    at 900, 000 images), they under/over-estimate the data requirement by
                    120, 000 to 310, 000 images.
                </p>
            </figure>
    </section>

    <section id="abstract">
        <h2>Abstract</h2>
        <hr>
        <p>
            Given a small training data set and a learning algorithm,
            how much more data is necessary to reach a target
            validation or test performance? This question is of critical
            importance in applications such as autonomous driving
            or medical imaging where collecting data is expensive and
            time-consuming. Overestimating or underestimating data
            requirements incurs substantial costs that could be avoided
            with an adequate budget. Prior work on neural scaling
            laws suggest that the power-law function can fit the validation
            performance curve and extrapolate it to larger data set
            sizes. We find that this does not immediately translate to the
            more difficult downstream task of estimating the required
            data set size to meet a target performance. In this work, we
            consider a broad class of computer vision tasks and systematically
            investigate a family of functions that generalize the
            power-law function to allow for better estimation of data
            requirements. Finally, we show that incorporating a tuned
            correction factor and collecting over multiple rounds significantly
            improves the performance of the data estimators.
            Using our guidelines, practitioners can accurately estimate
            data requirements of machine learning systems to gain savings
            in both development time and data acquisition costs.
        </p>
        
        <h3>Guidelines for collecting just enough data to meet performance targets</h3>
        <ul class="insight">
            <li>Account for multiple (e.g., five) rounds of sequential data collection. 
                It is hard to accurately estimate how much data is needed in one shot.</li>
            <li>Use regression functions that tend to under-estimate the data requirement 
                in order to avoid over-collecting data by large margins.
            </li>
            <li>If the desired performance is <i>V*</i>, estimate the data requirement 
                for <i>V*+&#964;</i>. Use previous tasks & data sets to 
                learn a good value for this correction factor.</li>
        </ul>
    </section>

    <section id="method">
    <h2>Simulation Method</h2>
    <hr>
        <p>
            Given a target performance <i>V*</i>, initial set of points <i>D<sub>0</sub></i>, and 
            maximum number of collection rounds <i>T</i>, we first partition the data set
            into subsets <i>S<sub>i</sub></i>, train our model, and evaluate the 
            performance <i>V<sub>f</sub>(S<sub>i</sub>)</i> with each subset. 
            We use this set of training statistics to fit a submodular regression function that 
            can extrapolate performance and thereby estimate how much data is needed to meet a 
            target score. Finally, we collect this additional data, train our model, and evaluate 
            the current performance. We repeat until we reach the target or after <i>T</i> rounds. 
        </p>
        <figure style="width: 90%;">
            <a href="assets/FlowChart.PNG">
                <img width="100%" src="assets/FlowChart.PNG">
            </a>
        </figure>
    </section>


    <section id="Results">
        <h2>Results</h2>
    <hr>
    <h3>Evaluating regression functions on whether they under/over-collect</h3>


        <p>
            We plot the ratio of the total amount of data collected over the minimum amount of data 
            needed (y-axis) for different target <i>V*</i> values (x-axis) in simulations 
            initializing with <i>n<sub>0</sub></i> = 10% of the data set (<i>n<sub>0</sub></i> = 20% 
            for VOC). We explore seven data sets covering classification, detection, and segmentation. For 
            each data set, we show simulations for <i>T</i> = 1, 3, 5 rounds. The dashed black line 
            corresponds to a ratio of 1, i.e., collecting exactly the minimum amount of data needed. 
            <b>We observe that the Arctan function consistently leads to over-collecting data (i.e., ratios greater than 1),
                whereas the other functions consistently lead to under-collecting data (i.e., ratios less than 1).
            </b>
        </p>
    <figure style="width: 99%;">
        <img width="16%" src="assets/simulations/simulation rel CIFAR10 T1, n10 Points 5 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel CIFAR10 T3, n10 Points 5 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel CIFAR10 T5, n10 Points 5 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel CIFAR100 T1, n10 Points 5 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel CIFAR100 T3, n10 Points 5 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel CIFAR100 T5, n10 Points 5 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel ImageNet T1, n10 Points 4 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel ImageNet T3, n10 Points 4 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel ImageNet T5, n10 Points 4 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel VOC T1, n20 Points 4 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel VOC T3, n20 Points 4 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel VOC T5, n20 Points 4 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel nuScenes Det. T1, n10 Points 2 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel nuScenes Det. T3, n10 Points 2 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel nuScenes Det. T5, n10 Points 2 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel BDD100K T1, n10 Points 5 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel BDD100K T3, n10 Points 5 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel BDD100K T5, n10 Points 5 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel nuScenes Seg. T1, n10 Points 5 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel nuScenes Seg. T3, n10 Points 5 Tol 0.png">
        <img width="16%" src="assets/simulations/simulation rel nuScenes Seg. T5, n10 Points 5 Tol 0.png">
    </figure>

    <hr>
    <h3>Incorporating a correction factor</h3>
    
    
        <p>
            If our target is <i>V*</i>, let us instead collect data to meet <i>V*+&#964;</i>,
            where <i>&#964;</i> is a correction factor to promote over-collection. 
            We can learn a good value for this correction factor by simulating on previous tasks,
            e.g., use our experience on CIFAR10 to learn a factor for ImageNet. 
        </p>
        
            
        <p>    
            After incorporating a correction factor learned from CIFAR10, we plot the ratio of the total amount of data 
            collected over the minimum amount of data needed (y-axis) for different 
            target <i>V*</i> values (x-axis) in simulations. 
            <b>
            Using this correction factor allows the 
            under-collecting regression functions (i.e., Power Law, Logarithmic, Algebraic Root) to now
            collect slightly more than the minimum amount of data required (i.e., achieve ratios less than 2).
            </b>
        </p>
    
    <figure>
        <img width="16%" src="assets/simulations/simulation rel CIFAR100 T5, n10 Points 5 Tol -1.png">
        <img width="16%" src="assets/simulations/simulation rel ImageNet T5, n10 Points 4 Tol -1.png">
        <img width="16%" src="assets/simulations/simulation rel VOC T5, n20 Points 4 Tol -1.png">
        <img width="16%" src="assets/simulations/simulation rel nuScenes Det. T5, n10 Points 2 Tol -1.png">
        <img width="16%" src="assets/simulations/simulation rel BDD100K T5, n10 Points 5 Tol -1.png">
        <img width="16%" src="assets/simulations/simulation rel nuScenes Seg. T5, n10 Points 5 Tol -1.png">
    </figure>

    
    </section>
    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>
            @InProceedings{Mahmood_2022_CVPR,
                author    = {Mahmood, Rafid and Lucas, James and Acuna, David and Li, Daiqing 
                                and Philion, Jonah and Alvarez, Jose M. and Yu, Zhiding and Fidler, Sanja 
                                and Law, Marc T.},
                title     = {How Much More Data Do I Need? Estimating Requirements for Downstream Tasks},
                booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
                month     = {June},
                year      = {2022},
                pages     = {275-284}}
        </code></pre>
    </section>

    <section id="paper">
        <h2>Paper</h2>
        <hr>
        <figure style="width: 20%;">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Mahmood_How_Much_More_Data_Do_I_Need_Estimating_Requirements_for_CVPR_2022_paper.html">
                <img width="100%" src="assets/Paper_FrontPage.PNG">
            </a>
        </figure>
        <hr>
    </section>


<script type="module">
    import * as THREE from "https://unpkg.com/three@0.127.0/build/three.module.js";
    import {OrbitControls} from "https://unpkg.com/three@0.127.0/examples/jsm/controls/OrbitControls.js";
    import {OBJLoader} from "https://unpkg.com/three@0.127.0/examples/jsm/loaders/OBJLoader.js";

    // Render the predictions
    function random_choice(arr, n) {
        var index_set = {};
        var choice = [];
        while (choice.length < n) {
            var idx = Math.floor(Math.random() * arr.length);
            if (index_set[idx] !== undefined) {
                continue;
            }
            index_set[idx] = 0;
            choice.push(idx);
        }

        return choice.map(x => arr[x]);
    }

    function progress_bar() {
        var el = document.createElement("div");
        el.classList.add("progress");

        return {
            domElement: el,
            update: function (percent) {
                percent = Math.min(1, Math.max(0, percent));
                el.style.display = "block";
                el.style.width = Math.round(percent * 100) + "%";
            },
            hide: function () {
                el.style.display = "none";
            }
        };
    }

    function reset_checkboxes(checkboxes) {
        Array.prototype.forEach.call(checkboxes, function (c) {
            c.checked = false;
        });
        checkboxes[0].checked = true;
        checkboxes[1].checked = true;
    }

    function show_object(el, prefix, N) {
        const scene = new THREE.Scene();
        const renderer = new THREE.WebGLRenderer();
        const camera = new THREE.PerspectiveCamera(75, 1, 0.1, 1000);
        const controls = new OrbitControls(camera, renderer.domElement);

        camera.position.set(0.55, 0.55, 0.55);
        controls.target.set(0, 0, 0);
        controls.autoRotate = true;
        controls.autoRotateSpeed = 4;
        scene.background = new THREE.Color("white");
        var size = el.dataset.size;
        renderer.setSize(size, size);
        var progress = progress_bar();
        el.appendChild(progress.domElement);
        el.appendChild(renderer.domElement);

        const spotLight = new THREE.SpotLight( 0x909090 );
        spotLight.position.set( -100, -1000, -100 );

        spotLight.castShadow = true;
        scene.add( spotLight );

        const spotLight_2 = new THREE.SpotLight( 0x909090 );
        spotLight_2.position.set(100, 1000, 100 );

        spotLight_2.castShadow = true;
        scene.add( spotLight_2 );

        const spotLight_3= new THREE.SpotLight( 0x909090 );
        spotLight_3.position.set(0, 0, 100000 );

        spotLight_3.castShadow = true;
        scene.add( spotLight_3);

        const spotLight_4= new THREE.SpotLight( 0x909090 );
        spotLight_4.position.set(0, 0, -100000 );

        spotLight_4.castShadow = true;
        scene.add( spotLight_4);

        const colors = [
            0xf4f4f4,
            0xbc96dc,
        ];

        var previous_canvas_size = size;
        function animate() {
            requestAnimationFrame(animate);
            if (el.offsetWidth != previous_canvas_size) {
                previous_canvas_size = el.offsetWidth;
                renderer.domElement.style.width = previous_canvas_size + "px";
                renderer.domElement.style.height = previous_canvas_size + "px";
            }

            controls.update();
            renderer.render(scene, camera);
        }

        const loader = new OBJLoader();
        var meshes = [];
        var progresses = [];
        var loaded = 0;
        function load_part(part_idx) {
            progresses[part_idx] = 0;
            loader.load(
                prefix + "/part_00" + i + ".obj",
                function (object) {
                    var g = object.children[0].geometry;
                    var m = new THREE.MeshLambertMaterial({color: colors[part_idx]});
                    m.side = THREE.DoubleSide;
                    //g.computeVertexNormals();
                    var mesh = new THREE.Mesh(g, m);
                    meshes[part_idx] = mesh;

                    scene.add(mesh);

                    loaded++;
                    if (loaded == N) {
                        progress.hide();
                    }
                },
                function (event) {
                    progresses[part_idx] = event.loaded / event.total;
                    var total_progress = 0;
                    for (var i=0; i<progresses.length; i++) {
                        total_progress += progresses[i] / progresses.length;
                    }
                    progress.update(total_progress);
                }
            )
        }
        for (var i=0; i<N; i++) {
            load_part(i);
        }
        animate();

        return {
            meshes: meshes,
            show: function (indices) {
                for (var i=0; i<N; i++) {
                    //meshes[i].material.opacity = 0.0;
                    meshes[i].visible = false;
                }
                for (var i=0; i<indices.length; i++) {
                    //meshes[indices[i]].material.opacity = 1;
                    meshes[indices[i]].visible = true;
                }
            },
            show_all: function () {
                for (var i=0; i<N; i++) {
                    //meshes[i].material.opacity = 1;
                    meshes[i].visible = true;
                }
            },
            set_size: function(width, height) {
                renderer.setSize(width, height);
            }
        };
    }


    function show_group(elements, objects, N) {
        var controls = [];
        for (var i=0; i<elements.length; i++) {
            if (i==0) {
                var scene_name = 'https://raw.githubusercontent.com/nv-tlabs/nkf/main/assets/models/shapenet_reconstruction/conv_occnet/' + objects[0]
                controls.push(show_object(elements[i], scene_name, N));
            } else if (i==1) {
                var scene_name = 'https://raw.githubusercontent.com/nv-tlabs/nkf/main/assets/models/shapenet_reconstruction/ours/' + objects[0]
                controls.push(show_object(elements[i], scene_name, N));
            } else {
                var scene_name = 'https://raw.githubusercontent.com/nv-tlabs/nkf/main/assets/models/shapenet_reconstruction/ground_truth/'  + objects[0]
                controls.push(show_object(elements[i], scene_name, N));
            }
        }

        return {
            controls: controls,
            show: function (indices) {
                for (var i=0; i<controls.length; i++) {
                    controls[i].show(indices);
                }
            },
            show_all: function () {
                for (var i=0; i<controls.length; i++) {
                    controls[i].show_all();
                }
            }
        };
    }

    var shapenet = [
    "40",
    "1320",
    "1560",
    "1680",
    "2000",
    "2880",
    "3120",
    "3700",
    "3780",
    "4260",
    "4960",
    "5020",
    "5440",
    "6060",
    "6960",
    "7740",
    "8620",
    ];

    var shapenet_control = show_group(
        document.getElementById("shapenet").getElementsByClassName("render_window"),
        [shapenet[12], shapenet[1], shapenet[2]],
        2
    );
    var shapenet_checkboxes = document.querySelectorAll("#shapenet .controls input");
    reset_checkboxes(shapenet_checkboxes);
    document.querySelector("#shapenet .controls").addEventListener(
        "change",
        function (ev) {

            var ids = new Set();
            var part_ids = [0, 1];
            for (var i=0; i<shapenet_checkboxes.length; i++) {
                if (shapenet_checkboxes[i].checked) {
                    ids.add(part_ids[i]);
                }
            }

            shapenet_control.show(Array.from(ids));
        }
    );
    document.querySelector("#shapenet .controls button").addEventListener(
        "click",
        function (ev) {
            reset_checkboxes(shapenet_checkboxes);
            var new_shapenet = random_choice(shapenet, 1);
            var render_windows = document.getElementById("shapenet").getElementsByClassName("render_window");
            Array.prototype.forEach.call(render_windows, function (r) {r.innerHTML = "";});
            shapenet_control = show_group(
                render_windows,
                new_shapenet,
                2
            );
        }
    );



</script>
</body>
</html>